{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeOqcOqEIHo5L6ve6xPBIa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PMJF3ybMk0cN",
        "outputId": "0e534159-651e-4347-8843-606f17801e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shap\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"dropout.csv\")\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Define features and target (assuming last column is the target)\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Map target labels to class names\n",
        "class_labels = {0: \"graduate\", 1: \"dropout\", 2: \"enrolled\"}\n",
        "y = y.map(class_labels)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "-0iflOPok-1p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X is non-negative for chi2 (Modified #1 Preprocessing)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Feature Selection (Modified #1 - SelectKBest)\n",
        "selector = SelectKBest(chi2, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Synthetic Oversampling (Modified #1 - Handling Class Imbalance)\n",
        "smote = SMOTE(random_state=318945)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=318945)\n",
        "\n",
        "# Polynomial Features (Modified #1 - Feature Engineering for Logistic Regression)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)"
      ],
      "metadata": {
        "id": "lhH00OSvlIq1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train, evaluate and return metrics\n",
        "def train_and_evaluate(model, model_name, X_train, X_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[\"graduate\", \"dropout\", \"enrolled\"])\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"graduate\", \"dropout\", \"enrolled\"])\n",
        "\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Baseline Models\n",
        "rf_baseline = train_and_evaluate(RandomForestClassifier(random_state=318945), \"Random Forest Baseline\", X_train, X_test)\n",
        "log_reg_baseline = train_and_evaluate(LogisticRegression(random_state=318945), \"Logistic Regression Baseline\", X_train, X_test)\n",
        "svm_baseline = train_and_evaluate(SVC(kernel=\"linear\", random_state=318945), \"SVM Baseline\", X_train, X_test)\n",
        "nb_baseline = train_and_evaluate(GaussianNB(), \"Naïve Bayes Baseline\", X_train, X_test)\n",
        "\n",
        "# Modified Algorithm #1 Models\n",
        "rf_modified1 = train_and_evaluate(RandomForestClassifier(min_samples_split=5, class_weight=\"balanced\", random_state=318945), \"Random Forest Modified #1\", X_train, X_test)\n",
        "log_reg_modified1 = train_and_evaluate(LogisticRegression(C=0.5, max_iter=1000, random_state=318945), \"Logistic Regression Modified #1\", X_train_poly, X_test_poly)\n",
        "svm_modified1 = train_and_evaluate(SVC(C=0.5, kernel=\"rbf\", random_state=318945), \"SVM Modified #1\", X_train, X_test)\n",
        "nb_modified1 = train_and_evaluate(GaussianNB(var_smoothing=1e-2), \"Naïve Bayes Modified #1\", X_train, X_test)\n",
        "\n",
        "# Hyperparameter optimization with Optuna (Modified Algorithm #2)\n",
        "def objective(trial):\n",
        "    model_name = trial.suggest_categorical(\"model\", [\"Random Forest\", \"Logistic Regression\", \"SVM\", \"Naïve Bayes\"])\n",
        "    if model_name == \"Random Forest\":\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            max_depth=trial.suggest_int(\"max_depth\", 3, 20),\n",
        "            random_state=318945\n",
        "        )\n",
        "    elif model_name == \"Logistic Regression\":\n",
        "        model = LogisticRegression(\n",
        "            C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
        "            max_iter=1000,\n",
        "            random_state=318945\n",
        "        )\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(\n",
        "            C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
        "            kernel=\"linear\",\n",
        "            random_state=318945\n",
        "        )\n",
        "    else:\n",
        "        model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "# Train and evaluate Optimized Models (Modified #2)\n",
        "rf_optimized = RandomForestClassifier(n_estimators=study.best_params.get(\"n_estimators\", 100), max_depth=study.best_params.get(\"max_depth\", None), random_state=318945)\n",
        "log_reg_optimized = LogisticRegression(C=study.best_params.get(\"C\", 1.0), max_iter=1000, random_state=318945)\n",
        "svm_optimized = SVC(C=study.best_params.get(\"C\", 1.0), kernel=\"linear\", random_state=318945)\n",
        "nb_optimized = GaussianNB(var_smoothing=study.best_params.get(\"var_smoothing\", 1e-9))\n",
        "\n",
        "train_and_evaluate(rf_optimized, \"Random Forest Optimized\", X_train, X_test)\n",
        "train_and_evaluate(log_reg_optimized, \"Logistic Regression Optimized\", X_train_poly, X_test_poly)\n",
        "train_and_evaluate(svm_optimized, \"SVM Optimized\", X_train, X_test)\n",
        "train_and_evaluate(nb_optimized, \"Naïve Bayes Optimized\", X_train, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oE03JZPGlK7m",
        "outputId": "b963829f-729a-4647-9539-baee48665a82"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Random Forest Baseline\n",
            "Accuracy: 0.6923\n",
            "F1 Score: 0.6916\n",
            "Confusion Matrix:\n",
            "[[335  75  48]\n",
            " [ 74 254  92]\n",
            " [ 47  72 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.60      0.62       420\n",
            "     dropout       0.70      0.73      0.72       448\n",
            "    enrolled       0.73      0.73      0.73       458\n",
            "\n",
            "    accuracy                           0.69      1326\n",
            "   macro avg       0.69      0.69      0.69      1326\n",
            "weighted avg       0.69      0.69      0.69      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Baseline\n",
            "Accuracy: 0.6320\n",
            "F1 Score: 0.6376\n",
            "Confusion Matrix:\n",
            "[[266 142  50]\n",
            " [ 38 255 127]\n",
            " [ 10 121 317]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.49      0.61      0.54       420\n",
            "     dropout       0.64      0.71      0.67       448\n",
            "    enrolled       0.85      0.58      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.66      0.63      0.64      1326\n",
            "weighted avg       0.67      0.63      0.64      1326\n",
            "\n",
            "\n",
            "Model: SVM Baseline\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Baseline\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n",
            "\n",
            "Model: Random Forest Modified #1\n",
            "Accuracy: 0.7119\n",
            "F1 Score: 0.7111\n",
            "Confusion Matrix:\n",
            "[[342  71  45]\n",
            " [ 71 259  90]\n",
            " [ 36  69 343]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.65      0.62      0.63       420\n",
            "     dropout       0.72      0.77      0.74       448\n",
            "    enrolled       0.76      0.75      0.75       458\n",
            "\n",
            "    accuracy                           0.71      1326\n",
            "   macro avg       0.71      0.71      0.71      1326\n",
            "weighted avg       0.71      0.71      0.71      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Modified #1\n",
            "Accuracy: 0.6350\n",
            "F1 Score: 0.6380\n",
            "Confusion Matrix:\n",
            "[[275 133  50]\n",
            " [ 55 235 130]\n",
            " [ 12 104 332]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.50      0.56      0.53       420\n",
            "     dropout       0.65      0.74      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.63      0.64      1326\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-20 03:08:09,270] A new study created in memory with name: no-name-d76862d3-b43a-4d4b-a6ed-e7ba947dd63b\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:08:09,288] Trial 0 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 2.3639868980947885e-09}. Best is trial 0 with value: 0.6231786518292287.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:09,341] Trial 1 finished with value: 0.6363243470518782 and parameters: {'model': 'Logistic Regression', 'C': 8.71842729269408}. Best is trial 1 with value: 0.6363243470518782.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:09,377] Trial 2 finished with value: 0.6322648257204074 and parameters: {'model': 'Logistic Regression', 'C': 0.016247730275884536}. Best is trial 1 with value: 0.6363243470518782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM Modified #1\n",
            "Accuracy: 0.6418\n",
            "F1 Score: 0.6432\n",
            "Confusion Matrix:\n",
            "[[276 132  50]\n",
            " [ 49 230 141]\n",
            " [ 16  87 345]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.55      0.53       420\n",
            "     dropout       0.64      0.77      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.66      0.64      0.64      1326\n",
            "weighted avg       0.66      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Modified #1\n",
            "Accuracy: 0.6214\n",
            "F1 Score: 0.6145\n",
            "Confusion Matrix:\n",
            "[[277 105  76]\n",
            " [ 55 174 191]\n",
            " [ 16  59 373]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.41      0.46       420\n",
            "     dropout       0.58      0.83      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.62      1326\n",
            "   macro avg       0.63      0.62      0.61      1326\n",
            "weighted avg       0.63      0.62      0.61      1326\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-20 03:08:10,408] Trial 3 finished with value: 0.6955776112988175 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 17}. Best is trial 3 with value: 0.6955776112988175.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:11,397] Trial 4 finished with value: 0.5354578536701117 and parameters: {'model': 'SVM', 'C': 0.017354129506948868}. Best is trial 3 with value: 0.6955776112988175.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:11,450] Trial 5 finished with value: 0.6392450150673481 and parameters: {'model': 'Logistic Regression', 'C': 1.4676972695610868}. Best is trial 3 with value: 0.6955776112988175.\n",
            "[I 2025-02-20 03:08:12,236] Trial 6 finished with value: 0.7125400309290673 and parameters: {'model': 'Random Forest', 'n_estimators': 152, 'max_depth': 13}. Best is trial 6 with value: 0.7125400309290673.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:20,126] Trial 7 finished with value: 0.6405803134320113 and parameters: {'model': 'SVM', 'C': 289.8627908943283}. Best is trial 6 with value: 0.7125400309290673.\n",
            "[I 2025-02-20 03:08:20,431] Trial 8 finished with value: 0.7081213359507499 and parameters: {'model': 'Random Forest', 'n_estimators': 59, 'max_depth': 12}. Best is trial 6 with value: 0.7125400309290673.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:21,270] Trial 9 finished with value: 0.6324962678079074 and parameters: {'model': 'SVM', 'C': 0.1599725873024304}. Best is trial 6 with value: 0.7125400309290673.\n",
            "[I 2025-02-20 03:08:21,761] Trial 10 finished with value: 0.6459615440823041 and parameters: {'model': 'Random Forest', 'n_estimators': 167, 'max_depth': 3}. Best is trial 6 with value: 0.7125400309290673.\n",
            "[I 2025-02-20 03:08:21,944] Trial 11 finished with value: 0.7078333559490976 and parameters: {'model': 'Random Forest', 'n_estimators': 35, 'max_depth': 11}. Best is trial 6 with value: 0.7125400309290673.\n",
            "[I 2025-02-20 03:08:22,342] Trial 12 finished with value: 0.7088478086146213 and parameters: {'model': 'Random Forest', 'n_estimators': 75, 'max_depth': 12}. Best is trial 6 with value: 0.7125400309290673.\n",
            "[I 2025-02-20 03:08:22,916] Trial 13 finished with value: 0.7141665159162729 and parameters: {'model': 'Random Forest', 'n_estimators': 112, 'max_depth': 12}. Best is trial 13 with value: 0.7141665159162729.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:08:22,940] Trial 14 finished with value: 0.6131635721537138 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.02849397255675797}. Best is trial 13 with value: 0.7141665159162729.\n",
            "[I 2025-02-20 03:08:23,671] Trial 15 finished with value: 0.6939453838305861 and parameters: {'model': 'Random Forest', 'n_estimators': 126, 'max_depth': 17}. Best is trial 13 with value: 0.7141665159162729.\n",
            "[I 2025-02-20 03:08:24,172] Trial 16 finished with value: 0.6800722680908395 and parameters: {'model': 'Random Forest', 'n_estimators': 124, 'max_depth': 8}. Best is trial 13 with value: 0.7141665159162729.\n",
            "[I 2025-02-20 03:08:24,981] Trial 17 finished with value: 0.7150726852484167 and parameters: {'model': 'Random Forest', 'n_estimators': 147, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:08:25,005] Trial 18 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 3.5271478435352385e-06}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:25,417] Trial 19 finished with value: 0.6816413828707307 and parameters: {'model': 'Random Forest', 'n_estimators': 97, 'max_depth': 8}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:26,235] Trial 20 finished with value: 0.6895607881811429 and parameters: {'model': 'Random Forest', 'n_estimators': 138, 'max_depth': 20}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:27,127] Trial 21 finished with value: 0.7075744534958609 and parameters: {'model': 'Random Forest', 'n_estimators': 159, 'max_depth': 15}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:28,196] Trial 22 finished with value: 0.7134516101768669 and parameters: {'model': 'Random Forest', 'n_estimators': 192, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:29,249] Trial 23 finished with value: 0.7061281461971937 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 15}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:31,231] Trial 24 finished with value: 0.7068274588753998 and parameters: {'model': 'Random Forest', 'n_estimators': 197, 'max_depth': 15}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:32,346] Trial 25 finished with value: 0.6987975969919802 and parameters: {'model': 'Random Forest', 'n_estimators': 106, 'max_depth': 10}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:32,383] Trial 26 finished with value: 0.5907152036010442 and parameters: {'model': 'Logistic Regression', 'C': 0.0011576862701350246}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:08:32,406] Trial 27 finished with value: 0.606405244700731 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.07968691689198101}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:51,676] Trial 28 finished with value: 0.6405803134320113 and parameters: {'model': 'SVM', 'C': 940.6342983652285}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:08:51,700] Trial 29 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 2.344152195681267e-09}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:52,139] Trial 30 finished with value: 0.6842617225632796 and parameters: {'model': 'Random Forest', 'n_estimators': 101, 'max_depth': 9}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:52,921] Trial 31 finished with value: 0.7109794480523528 and parameters: {'model': 'Random Forest', 'n_estimators': 148, 'max_depth': 13}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:53,837] Trial 32 finished with value: 0.7126675505794124 and parameters: {'model': 'Random Forest', 'n_estimators': 169, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:54,820] Trial 33 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 183, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:54,883] Trial 34 finished with value: 0.6363243470518782 and parameters: {'model': 'Logistic Regression', 'C': 31.006771080617543}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:56,511] Trial 35 finished with value: 0.6946849931461421 and parameters: {'model': 'Random Forest', 'n_estimators': 198, 'max_depth': 17}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:57,906] Trial 36 finished with value: 0.6976735818090484 and parameters: {'model': 'Random Forest', 'n_estimators': 178, 'max_depth': 16}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:57,940] Trial 37 finished with value: 0.5896737187426964 and parameters: {'model': 'Logistic Regression', 'C': 0.0010736132613811972}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:08:58,813] Trial 38 finished with value: 0.6372160238197574 and parameters: {'model': 'SVM', 'C': 0.2854957232923432}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:08:59,576] Trial 39 finished with value: 0.6926206262590702 and parameters: {'model': 'Random Forest', 'n_estimators': 131, 'max_depth': 19}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:00,524] Trial 40 finished with value: 0.7117950293409501 and parameters: {'model': 'Random Forest', 'n_estimators': 181, 'max_depth': 13}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:01,427] Trial 41 finished with value: 0.7150563251262461 and parameters: {'model': 'Random Forest', 'n_estimators': 166, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:02,311] Trial 42 finished with value: 0.7127494220058082 and parameters: {'model': 'Random Forest', 'n_estimators': 162, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:03,006] Trial 43 finished with value: 0.7087428888949588 and parameters: {'model': 'Random Forest', 'n_estimators': 144, 'max_depth': 11}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:03,984] Trial 44 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 181, 'max_depth': 14}. Best is trial 17 with value: 0.7150726852484167.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:06,882] Trial 45 finished with value: 0.6434299386938617 and parameters: {'model': 'SVM', 'C': 77.90359899636353}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:07,591] Trial 46 finished with value: 0.6947691461919593 and parameters: {'model': 'Random Forest', 'n_estimators': 116, 'max_depth': 16}. Best is trial 17 with value: 0.7150726852484167.\n",
            "[I 2025-02-20 03:09:08,932] Trial 47 finished with value: 0.7158562638221566 and parameters: {'model': 'Random Forest', 'n_estimators': 175, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:09,022] Trial 48 finished with value: 0.6376247379548496 and parameters: {'model': 'Logistic Regression', 'C': 4.234599827849728}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:10,368] Trial 49 finished with value: 0.6960472276188832 and parameters: {'model': 'Random Forest', 'n_estimators': 173, 'max_depth': 18}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:09:10,393] Trial 50 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 2.915814065101054e-05}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:11,290] Trial 51 finished with value: 0.7121425518106343 and parameters: {'model': 'Random Forest', 'n_estimators': 182, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:12,104] Trial 52 finished with value: 0.7095054464864003 and parameters: {'model': 'Random Forest', 'n_estimators': 161, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:13,080] Trial 53 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 182, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:14,078] Trial 54 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 185, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:14,957] Trial 55 finished with value: 0.6973825501466081 and parameters: {'model': 'Random Forest', 'n_estimators': 155, 'max_depth': 16}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:15,911] Trial 56 finished with value: 0.7082496156368809 and parameters: {'model': 'Random Forest', 'n_estimators': 173, 'max_depth': 15}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:16,804] Trial 57 finished with value: 0.7118331341032668 and parameters: {'model': 'Random Forest', 'n_estimators': 170, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:17,689] Trial 58 finished with value: 0.7078598300031946 and parameters: {'model': 'Random Forest', 'n_estimators': 187, 'max_depth': 11}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:18,580] Trial 59 finished with value: 0.5551177431583436 and parameters: {'model': 'SVM', 'C': 0.029524199713510786}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:19,569] Trial 60 finished with value: 0.7142219498870891 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:20,837] Trial 61 finished with value: 0.7134326993408875 and parameters: {'model': 'Random Forest', 'n_estimators': 198, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:22,058] Trial 62 finished with value: 0.6993027463588889 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 10}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:22,843] Trial 63 finished with value: 0.6631726765652454 and parameters: {'model': 'Random Forest', 'n_estimators': 188, 'max_depth': 5}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:22,999] Trial 64 finished with value: 0.7129526734204354 and parameters: {'model': 'Random Forest', 'n_estimators': 25, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:23,905] Trial 65 finished with value: 0.7127246014623161 and parameters: {'model': 'Random Forest', 'n_estimators': 168, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:09:23,928] Trial 66 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 3.3672445487903e-06}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:24,696] Trial 67 finished with value: 0.7129704126264739 and parameters: {'model': 'Random Forest', 'n_estimators': 153, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:25,802] Trial 68 finished with value: 0.7053102294294942 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 15}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:25,858] Trial 69 finished with value: 0.6370693357405999 and parameters: {'model': 'Logistic Regression', 'C': 0.31947735191759075}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:26,169] Trial 70 finished with value: 0.705790798414722 and parameters: {'model': 'Random Forest', 'n_estimators': 61, 'max_depth': 11}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:27,186] Trial 71 finished with value: 0.7134009281549691 and parameters: {'model': 'Random Forest', 'n_estimators': 186, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:28,153] Trial 72 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 175, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:29,129] Trial 73 finished with value: 0.7117092685165186 and parameters: {'model': 'Random Forest', 'n_estimators': 186, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:29,627] Trial 74 finished with value: 0.70289172632466 and parameters: {'model': 'Random Forest', 'n_estimators': 88, 'max_depth': 15}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:30,548] Trial 75 finished with value: 0.6961564794924189 and parameters: {'model': 'Random Forest', 'n_estimators': 161, 'max_depth': 16}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:31,573] Trial 76 finished with value: 0.7134009281549691 and parameters: {'model': 'Random Forest', 'n_estimators': 190, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:32,478] Trial 77 finished with value: 0.711959806927147 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:35,359] Trial 78 finished with value: 0.6405803134320113 and parameters: {'model': 'SVM', 'C': 31.526565469691587}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:09:35,384] Trial 79 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.00014704732768555214}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:36,391] Trial 80 finished with value: 0.7110290821947479 and parameters: {'model': 'Random Forest', 'n_estimators': 193, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:37,332] Trial 81 finished with value: 0.7134583009788604 and parameters: {'model': 'Random Forest', 'n_estimators': 173, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:38,296] Trial 82 finished with value: 0.7134745879439532 and parameters: {'model': 'Random Forest', 'n_estimators': 176, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:39,209] Trial 83 finished with value: 0.7084356233040279 and parameters: {'model': 'Random Forest', 'n_estimators': 163, 'max_depth': 15}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:40,185] Trial 84 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 183, 'max_depth': 14}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:40,937] Trial 85 finished with value: 0.7110059559145926 and parameters: {'model': 'Random Forest', 'n_estimators': 144, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:40,975] Trial 86 finished with value: 0.6194069134958797 and parameters: {'model': 'Logistic Regression', 'C': 0.0035854619731903905}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:42,035] Trial 87 finished with value: 0.7068061349377716 and parameters: {'model': 'Random Forest', 'n_estimators': 193, 'max_depth': 15}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:42,987] Trial 88 finished with value: 0.696856915764089 and parameters: {'model': 'Random Forest', 'n_estimators': 168, 'max_depth': 16}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:43,784] Trial 89 finished with value: 0.7014374495946488 and parameters: {'model': 'Random Forest', 'n_estimators': 176, 'max_depth': 10}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:44,738] Trial 90 finished with value: 0.7142144739671493 and parameters: {'model': 'Random Forest', 'n_estimators': 194, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:45,992] Trial 91 finished with value: 0.7149472998969232 and parameters: {'model': 'Random Forest', 'n_estimators': 192, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:47,469] Trial 92 finished with value: 0.7135182056650806 and parameters: {'model': 'Random Forest', 'n_estimators': 196, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:48,392] Trial 93 finished with value: 0.7116254582882275 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 11}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:49,343] Trial 94 finished with value: 0.7102774016001986 and parameters: {'model': 'Random Forest', 'n_estimators': 184, 'max_depth': 13}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:50,262] Trial 95 finished with value: 0.7093887240631167 and parameters: {'model': 'Random Forest', 'n_estimators': 195, 'max_depth': 11}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 03:09:51,131] Trial 96 finished with value: 0.573946900617878 and parameters: {'model': 'SVM', 'C': 0.06336835312234709}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:52,383] Trial 97 finished with value: 0.7121425518106343 and parameters: {'model': 'Random Forest', 'n_estimators': 182, 'max_depth': 12}. Best is trial 47 with value: 0.7158562638221566.\n",
            "[I 2025-02-20 03:09:53,276] Trial 98 finished with value: 0.7017304869530057 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 10}. Best is trial 47 with value: 0.7158562638221566.\n",
            "<ipython-input-21-2a842aef8e3b>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 03:09:53,301] Trial 99 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 7.441432782865073e-08}. Best is trial 47 with value: 0.7158562638221566.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'model': 'Random Forest', 'n_estimators': 175, 'max_depth': 12}\n",
            "\n",
            "Model: Random Forest Optimized\n",
            "Accuracy: 0.7142\n",
            "F1 Score: 0.7159\n",
            "Confusion Matrix:\n",
            "[[327  85  46]\n",
            " [ 50 287  83]\n",
            " [ 28  87 333]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.68      0.65       420\n",
            "     dropout       0.72      0.74      0.73       448\n",
            "    enrolled       0.81      0.71      0.76       458\n",
            "\n",
            "    accuracy                           0.71      1326\n",
            "   macro avg       0.72      0.71      0.71      1326\n",
            "weighted avg       0.72      0.71      0.72      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Optimized\n",
            "Accuracy: 0.6463\n",
            "F1 Score: 0.6492\n",
            "Confusion Matrix:\n",
            "[[273 135  50]\n",
            " [ 53 249 118]\n",
            " [ 12 101 335]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.59      0.55       420\n",
            "     dropout       0.67      0.75      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.65      1326\n",
            "   macro avg       0.66      0.65      0.65      1326\n",
            "weighted avg       0.67      0.65      0.65      1326\n",
            "\n",
            "\n",
            "Model: SVM Optimized\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Optimized\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GaussianNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.GaussianNB.html\">?<span>Documentation for GaussianNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model performance data\n",
        "data = {\n",
        "    \"Model\": [\"Random Forest\", \"Logistic Regression\", \"SVM\", \"Naïve Bayes\"],\n",
        "    \"Accuracy (Baseline)\": [69.23, 63.20, 63.57, 63.05],\n",
        "    \"F1 Score (Baseline)\": [69.16, 63.76, 63.86, 62.32],\n",
        "    \"Accuracy (Modified #1)\": [71.19, 63.50, 64.18, 62.14],\n",
        "    \"F1 Score (Modified #1)\": [71.11, 63.80, 64.32, 61.45],\n",
        "    \"Accuracy (Modified #2)\": [71.42, 64.63, 63.57, 63.05],\n",
        "    \"F1 Score (Modified #2)\": [71.59, 64.92, 63.86, 62.32],\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display table\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSkl2D-PVrix",
        "outputId": "1436ae24-9d67-43a8-d9ea-a0c4d6746d8f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Model  Accuracy (Baseline)  F1 Score (Baseline)  Accuracy (Modified #1)  F1 Score (Modified #1)  Accuracy (Modified #2)  F1 Score (Modified #2)\n",
            "      Random Forest                69.23                69.16                   71.19                   71.11                   71.42                   71.59\n",
            "Logistic Regression                63.20                63.76                   63.50                   63.80                   64.63                   64.92\n",
            "                SVM                63.57                63.86                   64.18                   64.32                   63.57                   63.86\n",
            "        Naïve Bayes                63.05                62.32                   62.14                   61.45                   63.05                   62.32\n"
          ]
        }
      ]
    }
  ]
}