{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS9Taf7w9m92ejCBNXyhAc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PMJF3ybMk0cN",
        "outputId": "0e534159-651e-4347-8843-606f17801e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shap\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"dropout.csv\")\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Define features and target (assuming last column is the target)\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Map target labels to class names\n",
        "class_labels = {0: \"graduate\", 1: \"dropout\", 2: \"enrolled\"}\n",
        "y = y.map(class_labels)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "-0iflOPok-1p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X is non-negative for chi2 (Modified #1 Preprocessing)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Feature Selection (Modified #1 - SelectKBest)\n",
        "selector = SelectKBest(chi2, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Synthetic Oversampling (Modified #1 - Handling Class Imbalance)\n",
        "smote = SMOTE(random_state=318945)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=318945)\n",
        "\n",
        "# Polynomial Features (Modified #1 - Feature Engineering for Logistic Regression)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)"
      ],
      "metadata": {
        "id": "lhH00OSvlIq1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train, evaluate and return metrics\n",
        "def train_and_evaluate(model, model_name, X_train, X_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[\"graduate\", \"dropout\", \"enrolled\"])\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"graduate\", \"dropout\", \"enrolled\"])\n",
        "\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Baseline Models\n",
        "rf_baseline = train_and_evaluate(RandomForestClassifier(random_state=318945), \"Random Forest Baseline\", X_train, X_test)\n",
        "log_reg_baseline = train_and_evaluate(LogisticRegression(random_state=318945), \"Logistic Regression Baseline\", X_train, X_test)\n",
        "svm_baseline = train_and_evaluate(SVC(kernel=\"linear\", random_state=318945), \"SVM Baseline\", X_train, X_test)\n",
        "nb_baseline = train_and_evaluate(GaussianNB(), \"Naïve Bayes Baseline\", X_train, X_test)\n",
        "\n",
        "# Modified Algorithm #1 Models\n",
        "rf_modified1 = train_and_evaluate(RandomForestClassifier(min_samples_split=5, class_weight=\"balanced\", random_state=318945), \"Random Forest Modified #1\", X_train, X_test)\n",
        "log_reg_modified1 = train_and_evaluate(LogisticRegression(C=0.5, max_iter=1000, random_state=318945), \"Logistic Regression Modified #1\", X_train_poly, X_test_poly)\n",
        "svm_modified1 = train_and_evaluate(SVC(C=0.5, kernel=\"rbf\", random_state=318945), \"SVM Modified #1\", X_train, X_test)\n",
        "nb_modified1 = train_and_evaluate(GaussianNB(var_smoothing=1e-2), \"Naïve Bayes Modified #1\", X_train, X_test)\n",
        "\n",
        "# Hyperparameter optimization with Optuna (Modified Algorithm #2)\n",
        "def objective(trial):\n",
        "    model_name = trial.suggest_categorical(\"model\", [\"Random Forest\", \"Logistic Regression\", \"SVM\", \"Naïve Bayes\"])\n",
        "    if model_name == \"Random Forest\":\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            max_depth=trial.suggest_int(\"max_depth\", 3, 20),\n",
        "            random_state=318945\n",
        "        )\n",
        "    elif model_name == \"Logistic Regression\":\n",
        "        model = LogisticRegression(\n",
        "            C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
        "            max_iter=1000,\n",
        "            random_state=318945\n",
        "        )\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(\n",
        "            C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
        "            kernel=\"linear\",\n",
        "            random_state=318945\n",
        "        )\n",
        "    else:\n",
        "        model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "# Train and evaluate Optimized Models (Modified #2)\n",
        "rf_optimized = RandomForestClassifier(n_estimators=study.best_params.get(\"n_estimators\", 100), max_depth=study.best_params.get(\"max_depth\", None), random_state=318945)\n",
        "log_reg_optimized = LogisticRegression(C=study.best_params.get(\"C\", 1.0), max_iter=1000, random_state=318945)\n",
        "svm_optimized = SVC(C=study.best_params.get(\"C\", 1.0), kernel=\"linear\", random_state=318945)\n",
        "nb_optimized = GaussianNB(var_smoothing=study.best_params.get(\"var_smoothing\", 1e-9))\n",
        "\n",
        "train_and_evaluate(rf_optimized, \"Random Forest Optimized\", X_train, X_test)\n",
        "train_and_evaluate(log_reg_optimized, \"Logistic Regression Optimized\", X_train_poly, X_test_poly)\n",
        "train_and_evaluate(svm_optimized, \"SVM Optimized\", X_train, X_test)\n",
        "train_and_evaluate(nb_optimized, \"Naïve Bayes Optimized\", X_train, X_test)\n",
        "\n",
        "# Dictionary to store accuracy scores\n",
        "accuracy_scores = {}\n",
        "\n",
        "# Baseline Models\n",
        "accuracy_scores[\"Random Forest Baseline\"] = train_and_evaluate(RandomForestClassifier(random_state=318945), \"Random Forest Baseline\", X_train, X_test)\n",
        "accuracy_scores[\"Logistic Regression Baseline\"] = train_and_evaluate(LogisticRegression(random_state=318945), \"Logistic Regression Baseline\", X_train, X_test)\n",
        "accuracy_scores[\"SVM Baseline\"] = train_and_evaluate(SVC(kernel=\"linear\", random_state=318945), \"SVM Baseline\", X_train, X_test)\n",
        "accuracy_scores[\"Naïve Bayes Baseline\"] = train_and_evaluate(GaussianNB(), \"Naïve Bayes Baseline\", X_train, X_test)\n",
        "\n",
        "# Modified Algorithm #1 Models\n",
        "accuracy_scores[\"Random Forest Modified #1\"] = train_and_evaluate(RandomForestClassifier(min_samples_split=5, class_weight=\"balanced\", random_state=318945), \"Random Forest Modified #1\", X_train, X_test)\n",
        "accuracy_scores[\"Logistic Regression Modified #1\"] = train_and_evaluate(LogisticRegression(C=0.5, max_iter=1000, random_state=318945), \"Logistic Regression Modified #1\", X_train_poly, X_test_poly)\n",
        "accuracy_scores[\"SVM Modified #1\"] = train_and_evaluate(SVC(C=0.5, kernel=\"rbf\", random_state=318945), \"SVM Modified #1\", X_train, X_test)\n",
        "accuracy_scores[\"Naïve Bayes Modified #1\"] = train_and_evaluate(GaussianNB(var_smoothing=1e-2), \"Naïve Bayes Modified #1\", X_train, X_test)\n",
        "\n",
        "# Train and evaluate Optimized Models (Modified #2)\n",
        "accuracy_scores[\"Random Forest Optimized\"] = train_and_evaluate(rf_optimized, \"Random Forest Optimized\", X_train, X_test)\n",
        "accuracy_scores[\"Logistic Regression Optimized\"] = train_and_evaluate(log_reg_optimized, \"Logistic Regression Optimized\", X_train_poly, X_test_poly)\n",
        "accuracy_scores[\"SVM Optimized\"] = train_and_evaluate(svm_optimized, \"SVM Optimized\", X_train, X_test)\n",
        "accuracy_scores[\"Naïve Bayes Optimized\"] = train_and_evaluate(nb_optimized, \"Naïve Bayes Optimized\", X_train, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oE03JZPGlK7m",
        "outputId": "32f9fbd7-8aef-4ce3-e198-3a7b500ba9a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Random Forest Baseline\n",
            "Accuracy: 0.6923\n",
            "F1 Score: 0.6916\n",
            "Confusion Matrix:\n",
            "[[335  75  48]\n",
            " [ 74 254  92]\n",
            " [ 47  72 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.60      0.62       420\n",
            "     dropout       0.70      0.73      0.72       448\n",
            "    enrolled       0.73      0.73      0.73       458\n",
            "\n",
            "    accuracy                           0.69      1326\n",
            "   macro avg       0.69      0.69      0.69      1326\n",
            "weighted avg       0.69      0.69      0.69      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Baseline\n",
            "Accuracy: 0.6320\n",
            "F1 Score: 0.6376\n",
            "Confusion Matrix:\n",
            "[[266 142  50]\n",
            " [ 38 255 127]\n",
            " [ 10 121 317]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.49      0.61      0.54       420\n",
            "     dropout       0.64      0.71      0.67       448\n",
            "    enrolled       0.85      0.58      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.66      0.63      0.64      1326\n",
            "weighted avg       0.67      0.63      0.64      1326\n",
            "\n",
            "\n",
            "Model: SVM Baseline\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Baseline\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n",
            "\n",
            "Model: Random Forest Modified #1\n",
            "Accuracy: 0.7119\n",
            "F1 Score: 0.7111\n",
            "Confusion Matrix:\n",
            "[[342  71  45]\n",
            " [ 71 259  90]\n",
            " [ 36  69 343]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.65      0.62      0.63       420\n",
            "     dropout       0.72      0.77      0.74       448\n",
            "    enrolled       0.76      0.75      0.75       458\n",
            "\n",
            "    accuracy                           0.71      1326\n",
            "   macro avg       0.71      0.71      0.71      1326\n",
            "weighted avg       0.71      0.71      0.71      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Modified #1\n",
            "Accuracy: 0.6350\n",
            "F1 Score: 0.6380\n",
            "Confusion Matrix:\n",
            "[[275 133  50]\n",
            " [ 55 235 130]\n",
            " [ 12 104 332]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.50      0.56      0.53       420\n",
            "     dropout       0.65      0.74      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.63      0.64      1326\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-20 02:56:33,011] A new study created in memory with name: no-name-b967e72c-819d-4810-9e21-612bc0de409e\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:33,030] Trial 0 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 8.488272602545059e-05}. Best is trial 0 with value: 0.6231786518292287.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:33,048] Trial 1 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.00011757245612105056}. Best is trial 0 with value: 0.6231786518292287.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM Modified #1\n",
            "Accuracy: 0.6418\n",
            "F1 Score: 0.6432\n",
            "Confusion Matrix:\n",
            "[[276 132  50]\n",
            " [ 49 230 141]\n",
            " [ 16  87 345]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.55      0.53       420\n",
            "     dropout       0.64      0.77      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.66      0.64      0.64      1326\n",
            "weighted avg       0.66      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Modified #1\n",
            "Accuracy: 0.6214\n",
            "F1 Score: 0.6145\n",
            "Confusion Matrix:\n",
            "[[277 105  76]\n",
            " [ 55 174 191]\n",
            " [ 16  59 373]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.41      0.46       420\n",
            "     dropout       0.58      0.83      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.62      1326\n",
            "   macro avg       0.63      0.62      0.61      1326\n",
            "weighted avg       0.63      0.62      0.61      1326\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-20 02:56:33,539] Trial 2 finished with value: 0.6889020948960678 and parameters: {'model': 'Random Forest', 'n_estimators': 83, 'max_depth': 19}. Best is trial 2 with value: 0.6889020948960678.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:33,593] Trial 3 finished with value: 0.6363243470518782 and parameters: {'model': 'Logistic Regression', 'C': 501.19325468591944}. Best is trial 2 with value: 0.6889020948960678.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:33,644] Trial 4 finished with value: 0.635423439957207 and parameters: {'model': 'Logistic Regression', 'C': 0.2256032664298982}. Best is trial 2 with value: 0.6889020948960678.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:33,697] Trial 5 finished with value: 0.6363243470518782 and parameters: {'model': 'Logistic Regression', 'C': 351.5619871818267}. Best is trial 2 with value: 0.6889020948960678.\n",
            "[I 2025-02-20 02:56:34,692] Trial 6 finished with value: 0.7134009281549691 and parameters: {'model': 'Random Forest', 'n_estimators': 186, 'max_depth': 14}. Best is trial 6 with value: 0.7134009281549691.\n",
            "[I 2025-02-20 02:56:35,038] Trial 7 finished with value: 0.7077630532234882 and parameters: {'model': 'Random Forest', 'n_estimators': 72, 'max_depth': 11}. Best is trial 6 with value: 0.7134009281549691.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:35,903] Trial 8 finished with value: 0.6295584343337058 and parameters: {'model': 'SVM', 'C': 0.14253025008127745}. Best is trial 6 with value: 0.7134009281549691.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:35,959] Trial 9 finished with value: 0.6385881083196671 and parameters: {'model': 'Logistic Regression', 'C': 0.6407089578776936}. Best is trial 6 with value: 0.7134009281549691.\n",
            "[I 2025-02-20 02:56:36,569] Trial 10 finished with value: 0.6459615440823041 and parameters: {'model': 'Random Forest', 'n_estimators': 197, 'max_depth': 3}. Best is trial 6 with value: 0.7134009281549691.\n",
            "[I 2025-02-20 02:56:36,766] Trial 11 finished with value: 0.7044700366638552 and parameters: {'model': 'Random Forest', 'n_estimators': 20, 'max_depth': 12}. Best is trial 6 with value: 0.7134009281549691.\n",
            "[I 2025-02-20 02:56:38,330] Trial 12 finished with value: 0.7117645885996609 and parameters: {'model': 'Random Forest', 'n_estimators': 199, 'max_depth': 13}. Best is trial 6 with value: 0.7134009281549691.\n",
            "[I 2025-02-20 02:56:39,634] Trial 13 finished with value: 0.7134256906849276 and parameters: {'model': 'Random Forest', 'n_estimators': 199, 'max_depth': 14}. Best is trial 13 with value: 0.7134256906849276.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:40,751] Trial 14 finished with value: 0.5406638977192532 and parameters: {'model': 'SVM', 'C': 0.004567351352320159}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:41,646] Trial 15 finished with value: 0.6952725282349792 and parameters: {'model': 'Random Forest', 'n_estimators': 155, 'max_depth': 17}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:42,442] Trial 16 finished with value: 0.7127821403029511 and parameters: {'model': 'Random Forest', 'n_estimators': 146, 'max_depth': 14}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:43,061] Trial 17 finished with value: 0.680007896633417 and parameters: {'model': 'Random Forest', 'n_estimators': 151, 'max_depth': 8}. Best is trial 13 with value: 0.7134256906849276.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:43,086] Trial 18 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 1.882596431254334e-09}. Best is trial 13 with value: 0.7134256906849276.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:44,610] Trial 19 finished with value: 0.6398163680387811 and parameters: {'model': 'SVM', 'C': 18.810768844192058}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:45,593] Trial 20 finished with value: 0.69760264415225 and parameters: {'model': 'Random Forest', 'n_estimators': 173, 'max_depth': 16}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:46,289] Trial 21 finished with value: 0.704707841610079 and parameters: {'model': 'Random Forest', 'n_estimators': 122, 'max_depth': 15}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:47,209] Trial 22 finished with value: 0.7126675505794124 and parameters: {'model': 'Random Forest', 'n_estimators': 169, 'max_depth': 14}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:47,803] Trial 23 finished with value: 0.7018717640753728 and parameters: {'model': 'Random Forest', 'n_estimators': 131, 'max_depth': 10}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:48,885] Trial 24 finished with value: 0.6969850478060593 and parameters: {'model': 'Random Forest', 'n_estimators': 186, 'max_depth': 18}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:49,759] Trial 25 finished with value: 0.6807528332973073 and parameters: {'model': 'Random Forest', 'n_estimators': 150, 'max_depth': 8}. Best is trial 13 with value: 0.7134256906849276.\n",
            "[I 2025-02-20 02:56:51,251] Trial 26 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 14}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:52,621] Trial 27 finished with value: 0.1820391356163699 and parameters: {'model': 'SVM', 'C': 0.0011813154886983663}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:52,644] Trial 28 finished with value: 0.607801252648366 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.07411081311645477}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:52,667] Trial 29 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 2.524965342999285e-09}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:53,674] Trial 30 finished with value: 0.6984195280700575 and parameters: {'model': 'Random Forest', 'n_estimators': 177, 'max_depth': 16}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:54,751] Trial 31 finished with value: 0.7134256906849276 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 14}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:55,757] Trial 32 finished with value: 0.7125604408344905 and parameters: {'model': 'Random Forest', 'n_estimators': 194, 'max_depth': 13}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:56,891] Trial 33 finished with value: 0.7053102294294942 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 15}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:57,685] Trial 34 finished with value: 0.6992635447232726 and parameters: {'model': 'Random Forest', 'n_estimators': 174, 'max_depth': 10}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:56:57,710] Trial 35 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 2.5339716824499605e-07}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:57,766] Trial 36 finished with value: 0.6369748256053119 and parameters: {'model': 'Logistic Regression', 'C': 10.526833106885034}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:58,671] Trial 37 finished with value: 0.711959806927147 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 12}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:56:59,555] Trial 38 finished with value: 0.7134996098299278 and parameters: {'model': 'Random Forest', 'n_estimators': 163, 'max_depth': 14}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:56:59,600] Trial 39 finished with value: 0.6314995516676789 and parameters: {'model': 'Logistic Regression', 'C': 0.03552074420342049}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:00,547] Trial 40 finished with value: 0.6930543883817536 and parameters: {'model': 'Random Forest', 'n_estimators': 163, 'max_depth': 20}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:01,538] Trial 41 finished with value: 0.714208509713412 and parameters: {'model': 'Random Forest', 'n_estimators': 185, 'max_depth': 14}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:04,502] Trial 42 finished with value: 0.6992357446747809 and parameters: {'model': 'Random Forest', 'n_estimators': 185, 'max_depth': 16}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:06,049] Trial 43 finished with value: 0.7103001015700114 and parameters: {'model': 'Random Forest', 'n_estimators': 165, 'max_depth': 13}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:07,158] Trial 44 finished with value: 0.7053102294294942 and parameters: {'model': 'Random Forest', 'n_estimators': 200, 'max_depth': 15}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:08,211] Trial 45 finished with value: 0.6954537846059672 and parameters: {'model': 'Random Forest', 'n_estimators': 184, 'max_depth': 17}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:08,268] Trial 46 finished with value: 0.6369748256053119 and parameters: {'model': 'Logistic Regression', 'C': 9.109607997200609}. Best is trial 26 with value: 0.714208509713412.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:09,246] Trial 47 finished with value: 0.5377836578816644 and parameters: {'model': 'SVM', 'C': 0.019312219218811946}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:09,880] Trial 48 finished with value: 0.7078406090255331 and parameters: {'model': 'Random Forest', 'n_estimators': 133, 'max_depth': 11}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:10,370] Trial 49 finished with value: 0.7101737293620171 and parameters: {'model': 'Random Forest', 'n_estimators': 89, 'max_depth': 14}. Best is trial 26 with value: 0.714208509713412.\n",
            "[I 2025-02-20 02:57:11,189] Trial 50 finished with value: 0.7150712613428691 and parameters: {'model': 'Random Forest', 'n_estimators': 163, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:12,026] Trial 51 finished with value: 0.7137313636461018 and parameters: {'model': 'Random Forest', 'n_estimators': 163, 'max_depth': 13}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:12,821] Trial 52 finished with value: 0.7137262490563986 and parameters: {'model': 'Random Forest', 'n_estimators': 160, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:13,630] Trial 53 finished with value: 0.7142814955378297 and parameters: {'model': 'Random Forest', 'n_estimators': 162, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:14,263] Trial 54 finished with value: 0.7040060649305176 and parameters: {'model': 'Random Forest', 'n_estimators': 138, 'max_depth': 10}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:14,520] Trial 55 finished with value: 0.7016481275339324 and parameters: {'model': 'Random Forest', 'n_estimators': 34, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:15,289] Trial 56 finished with value: 0.7100100771223766 and parameters: {'model': 'Random Forest', 'n_estimators': 106, 'max_depth': 11}. Best is trial 50 with value: 0.7150712613428691.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:18,887] Trial 57 finished with value: 0.645838010599004 and parameters: {'model': 'SVM', 'C': 81.73707668050083}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:19,587] Trial 58 finished with value: 0.6863551059820777 and parameters: {'model': 'Random Forest', 'n_estimators': 161, 'max_depth': 9}. Best is trial 50 with value: 0.7150712613428691.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:57:19,612] Trial 59 finished with value: 0.6060682440526691 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.0920120181909859}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:20,376] Trial 60 finished with value: 0.7103001015700114 and parameters: {'model': 'Random Forest', 'n_estimators': 142, 'max_depth': 13}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:21,182] Trial 61 finished with value: 0.7129001249187265 and parameters: {'model': 'Random Forest', 'n_estimators': 157, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:22,058] Trial 62 finished with value: 0.711094298261933 and parameters: {'model': 'Random Forest', 'n_estimators': 168, 'max_depth': 13}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:22,884] Trial 63 finished with value: 0.7103001015700114 and parameters: {'model': 'Random Forest', 'n_estimators': 157, 'max_depth': 13}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:23,395] Trial 64 finished with value: 0.6459615440823041 and parameters: {'model': 'Random Forest', 'n_estimators': 172, 'max_depth': 3}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:23,974] Trial 65 finished with value: 0.7077398047163439 and parameters: {'model': 'Random Forest', 'n_estimators': 122, 'max_depth': 11}. Best is trial 50 with value: 0.7150712613428691.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:24,032] Trial 66 finished with value: 0.6392450150673481 and parameters: {'model': 'Logistic Regression', 'C': 1.5605729081851225}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:24,793] Trial 67 finished with value: 0.7137593600489901 and parameters: {'model': 'Random Forest', 'n_estimators': 151, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:25,745] Trial 68 finished with value: 0.7149472998969232 and parameters: {'model': 'Random Forest', 'n_estimators': 190, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:26,702] Trial 69 finished with value: 0.6397611620056038 and parameters: {'model': 'SVM', 'C': 2.2561852977808146}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:27,993] Trial 70 finished with value: 0.7142144739671493 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:29,430] Trial 71 finished with value: 0.7142144739671493 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:30,266] Trial 72 finished with value: 0.6993027463588889 and parameters: {'model': 'Random Forest', 'n_estimators': 185, 'max_depth': 10}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:31,167] Trial 73 finished with value: 0.7116254582882275 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 11}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:32,114] Trial 74 finished with value: 0.7149472998969232 and parameters: {'model': 'Random Forest', 'n_estimators': 192, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:57:32,138] Trial 75 finished with value: 0.6231786518292287 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 6.356493453133497e-07}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:32,963] Trial 76 finished with value: 0.6862764151301096 and parameters: {'model': 'Random Forest', 'n_estimators': 190, 'max_depth': 9}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:33,901] Trial 77 finished with value: 0.7136876059812891 and parameters: {'model': 'Random Forest', 'n_estimators': 177, 'max_depth': 12}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:34,967] Trial 78 finished with value: 0.7068061349377716 and parameters: {'model': 'Random Forest', 'n_estimators': 193, 'max_depth': 15}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:35,921] Trial 79 finished with value: 0.7064590592343054 and parameters: {'model': 'Random Forest', 'n_estimators': 180, 'max_depth': 11}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:37,447] Trial 80 finished with value: 0.7134009281549691 and parameters: {'model': 'Random Forest', 'n_estimators': 191, 'max_depth': 14}. Best is trial 50 with value: 0.7150712613428691.\n",
            "[I 2025-02-20 02:57:38,313] Trial 81 finished with value: 0.7173369590821994 and parameters: {'model': 'Random Forest', 'n_estimators': 172, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:39,203] Trial 82 finished with value: 0.7127504013804677 and parameters: {'model': 'Random Forest', 'n_estimators': 178, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:40,541] Trial 83 finished with value: 0.7125418662173383 and parameters: {'model': 'Random Forest', 'n_estimators': 188, 'max_depth': 13}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:41,781] Trial 84 finished with value: 0.7056920120983308 and parameters: {'model': 'Random Forest', 'n_estimators': 169, 'max_depth': 11}. Best is trial 81 with value: 0.7173369590821994.\n",
            "<ipython-input-20-dc2053ec5bc1>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:41,840] Trial 85 finished with value: 0.5903804456764106 and parameters: {'model': 'Logistic Regression', 'C': 0.001068874937895857}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:42,445] Trial 86 finished with value: 0.662332512532372 and parameters: {'model': 'Random Forest', 'n_estimators': 182, 'max_depth': 4}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:43,401] Trial 87 finished with value: 0.7119182308565188 and parameters: {'model': 'Random Forest', 'n_estimators': 174, 'max_depth': 14}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:44,400] Trial 88 finished with value: 0.7110290821947479 and parameters: {'model': 'Random Forest', 'n_estimators': 193, 'max_depth': 13}. Best is trial 81 with value: 0.7173369590821994.\n",
            "<ipython-input-20-dc2053ec5bc1>:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  model = GaussianNB(var_smoothing=trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-1))\n",
            "[I 2025-02-20 02:57:44,425] Trial 89 finished with value: 0.6225739884532734 and parameters: {'model': 'Naïve Bayes', 'var_smoothing': 0.0013615875819000178}. Best is trial 81 with value: 0.7173369590821994.\n",
            "<ipython-input-20-dc2053ec5bc1>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C=trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
            "[I 2025-02-20 02:57:47,048] Trial 90 finished with value: 0.6355263814467882 and parameters: {'model': 'SVM', 'C': 78.56005250307668}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:47,807] Trial 91 finished with value: 0.7137593600489901 and parameters: {'model': 'Random Forest', 'n_estimators': 151, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:48,722] Trial 92 finished with value: 0.7136169253426642 and parameters: {'model': 'Random Forest', 'n_estimators': 183, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:49,502] Trial 93 finished with value: 0.6985869450448822 and parameters: {'model': 'Random Forest', 'n_estimators': 173, 'max_depth': 10}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:50,429] Trial 94 finished with value: 0.7094375622088943 and parameters: {'model': 'Random Forest', 'n_estimators': 198, 'max_depth': 11}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:51,406] Trial 95 finished with value: 0.7125418662173383 and parameters: {'model': 'Random Forest', 'n_estimators': 188, 'max_depth': 13}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:52,286] Trial 96 finished with value: 0.7137593600489901 and parameters: {'model': 'Random Forest', 'n_estimators': 151, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:54,329] Trial 97 finished with value: 0.7068274588753998 and parameters: {'model': 'Random Forest', 'n_estimators': 196, 'max_depth': 15}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:54,727] Trial 98 finished with value: 0.7091429207212371 and parameters: {'model': 'Random Forest', 'n_estimators': 62, 'max_depth': 14}. Best is trial 81 with value: 0.7173369590821994.\n",
            "[I 2025-02-20 02:57:55,566] Trial 99 finished with value: 0.7166447456859107 and parameters: {'model': 'Random Forest', 'n_estimators': 167, 'max_depth': 12}. Best is trial 81 with value: 0.7173369590821994.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'model': 'Random Forest', 'n_estimators': 172, 'max_depth': 12}\n",
            "\n",
            "Model: Random Forest Optimized\n",
            "Accuracy: 0.7157\n",
            "F1 Score: 0.7173\n",
            "Confusion Matrix:\n",
            "[[326  85  47]\n",
            " [ 49 289  82]\n",
            " [ 28  86 334]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.69      0.66       420\n",
            "     dropout       0.72      0.75      0.73       448\n",
            "    enrolled       0.81      0.71      0.76       458\n",
            "\n",
            "    accuracy                           0.72      1326\n",
            "   macro avg       0.72      0.72      0.72      1326\n",
            "weighted avg       0.72      0.72      0.72      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Optimized\n",
            "Accuracy: 0.6463\n",
            "F1 Score: 0.6492\n",
            "Confusion Matrix:\n",
            "[[273 135  50]\n",
            " [ 53 249 118]\n",
            " [ 12 101 335]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.59      0.55       420\n",
            "     dropout       0.67      0.75      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.65      1326\n",
            "   macro avg       0.66      0.65      0.65      1326\n",
            "weighted avg       0.67      0.65      0.65      1326\n",
            "\n",
            "\n",
            "Model: SVM Optimized\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Optimized\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n",
            "\n",
            "Model: Random Forest Baseline\n",
            "Accuracy: 0.6923\n",
            "F1 Score: 0.6916\n",
            "Confusion Matrix:\n",
            "[[335  75  48]\n",
            " [ 74 254  92]\n",
            " [ 47  72 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.60      0.62       420\n",
            "     dropout       0.70      0.73      0.72       448\n",
            "    enrolled       0.73      0.73      0.73       458\n",
            "\n",
            "    accuracy                           0.69      1326\n",
            "   macro avg       0.69      0.69      0.69      1326\n",
            "weighted avg       0.69      0.69      0.69      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Baseline\n",
            "Accuracy: 0.6320\n",
            "F1 Score: 0.6376\n",
            "Confusion Matrix:\n",
            "[[266 142  50]\n",
            " [ 38 255 127]\n",
            " [ 10 121 317]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.49      0.61      0.54       420\n",
            "     dropout       0.64      0.71      0.67       448\n",
            "    enrolled       0.85      0.58      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.66      0.63      0.64      1326\n",
            "weighted avg       0.67      0.63      0.64      1326\n",
            "\n",
            "\n",
            "Model: SVM Baseline\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Baseline\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n",
            "\n",
            "Model: Random Forest Modified #1\n",
            "Accuracy: 0.7119\n",
            "F1 Score: 0.7111\n",
            "Confusion Matrix:\n",
            "[[342  71  45]\n",
            " [ 71 259  90]\n",
            " [ 36  69 343]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.65      0.62      0.63       420\n",
            "     dropout       0.72      0.77      0.74       448\n",
            "    enrolled       0.76      0.75      0.75       458\n",
            "\n",
            "    accuracy                           0.71      1326\n",
            "   macro avg       0.71      0.71      0.71      1326\n",
            "weighted avg       0.71      0.71      0.71      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Modified #1\n",
            "Accuracy: 0.6350\n",
            "F1 Score: 0.6380\n",
            "Confusion Matrix:\n",
            "[[275 133  50]\n",
            " [ 55 235 130]\n",
            " [ 12 104 332]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.50      0.56      0.53       420\n",
            "     dropout       0.65      0.74      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.63      0.64      1326\n",
            "\n",
            "\n",
            "Model: SVM Modified #1\n",
            "Accuracy: 0.6418\n",
            "F1 Score: 0.6432\n",
            "Confusion Matrix:\n",
            "[[276 132  50]\n",
            " [ 49 230 141]\n",
            " [ 16  87 345]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.55      0.53       420\n",
            "     dropout       0.64      0.77      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.66      0.64      0.64      1326\n",
            "weighted avg       0.66      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Modified #1\n",
            "Accuracy: 0.6214\n",
            "F1 Score: 0.6145\n",
            "Confusion Matrix:\n",
            "[[277 105  76]\n",
            " [ 55 174 191]\n",
            " [ 16  59 373]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.41      0.46       420\n",
            "     dropout       0.58      0.83      0.69       448\n",
            "    enrolled       0.80      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.62      1326\n",
            "   macro avg       0.63      0.62      0.61      1326\n",
            "weighted avg       0.63      0.62      0.61      1326\n",
            "\n",
            "\n",
            "Model: Random Forest Optimized\n",
            "Accuracy: 0.7157\n",
            "F1 Score: 0.7173\n",
            "Confusion Matrix:\n",
            "[[326  85  47]\n",
            " [ 49 289  82]\n",
            " [ 28  86 334]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.63      0.69      0.66       420\n",
            "     dropout       0.72      0.75      0.73       448\n",
            "    enrolled       0.81      0.71      0.76       458\n",
            "\n",
            "    accuracy                           0.72      1326\n",
            "   macro avg       0.72      0.72      0.72      1326\n",
            "weighted avg       0.72      0.72      0.72      1326\n",
            "\n",
            "\n",
            "Model: Logistic Regression Optimized\n",
            "Accuracy: 0.6463\n",
            "F1 Score: 0.6492\n",
            "Confusion Matrix:\n",
            "[[273 135  50]\n",
            " [ 53 249 118]\n",
            " [ 12 101 335]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.59      0.55       420\n",
            "     dropout       0.67      0.75      0.70       448\n",
            "    enrolled       0.81      0.60      0.69       458\n",
            "\n",
            "    accuracy                           0.65      1326\n",
            "   macro avg       0.66      0.65      0.65      1326\n",
            "weighted avg       0.67      0.65      0.65      1326\n",
            "\n",
            "\n",
            "Model: SVM Optimized\n",
            "Accuracy: 0.6357\n",
            "F1 Score: 0.6386\n",
            "Confusion Matrix:\n",
            "[[278 128  52]\n",
            " [ 54 236 130]\n",
            " [ 16 103 329]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.51      0.56      0.53       420\n",
            "     dropout       0.64      0.73      0.69       448\n",
            "    enrolled       0.80      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.64      1326\n",
            "   macro avg       0.65      0.63      0.64      1326\n",
            "weighted avg       0.65      0.64      0.64      1326\n",
            "\n",
            "\n",
            "Model: Naïve Bayes Optimized\n",
            "Accuracy: 0.6305\n",
            "F1 Score: 0.6232\n",
            "Confusion Matrix:\n",
            "[[281 102  75]\n",
            " [ 56 178 186]\n",
            " [ 17  54 377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    graduate       0.53      0.42      0.47       420\n",
            "     dropout       0.59      0.84      0.69       448\n",
            "    enrolled       0.79      0.61      0.69       458\n",
            "\n",
            "    accuracy                           0.63      1326\n",
            "   macro avg       0.64      0.63      0.62      1326\n",
            "weighted avg       0.64      0.63      0.62      1326\n",
            "\n"
          ]
        }
      ]
    }
  ]
}